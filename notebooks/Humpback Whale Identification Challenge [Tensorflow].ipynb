{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competition URL\n",
    "\n",
    "https://www.kaggle.com/c/whale-categorization-playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all to RGB\n",
    "\n",
    "img_list = []\n",
    "img_names = os.listdir(\"train/train\")\n",
    "for img_name in (os.listdir(\"train/train\")):\n",
    "    img_list.append(\"train/train/\" + img_name)\n",
    "    \n",
    "i = 0\n",
    "for img in img_list:\n",
    "    img = Image.open(img)\n",
    "    rgbimg = Image.new(\"RGB\", img.size)\n",
    "    rgbimg.paste(img)\n",
    "    rgbimg.save('new_train/{}'.format(img_names[i]))\n",
    "    i += 1    \n",
    "    \n",
    "img_list = []\n",
    "img_names = os.listdir(\"test/test\")\n",
    "for img_name in (os.listdir(\"test/test\")):\n",
    "    img_list.append(\"test/test/\" + img_name)\n",
    "    \n",
    "i = 0\n",
    "for img in img_list:\n",
    "    img = Image.open(img)\n",
    "    rgbimg = Image.new(\"RGB\", img.size)\n",
    "    rgbimg.paste(img)\n",
    "    rgbimg.save('new_test/{}'.format(img_names[i]))\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting output labels\n",
    "\n",
    "def get_labels(predictions):\n",
    "    \n",
    "    input_train_file = \"train.csv\"\n",
    "    train_dataframe = pd.read_csv(input_train_file, header = 0)\n",
    "    train_labels = train_dataframe[\"Id\"].as_matrix()\n",
    "\n",
    "    labels_dict = {}\n",
    "    counter = 0\n",
    "    for i in range(9850):\n",
    "        if train_labels[i] in labels_dict:\n",
    "            labels_dict[train_labels[i]][1] += 1\n",
    "        else:\n",
    "            labels_dict[train_labels[i]] = [counter, 1]\n",
    "            counter += 1\n",
    "            \n",
    "    largest = heapq.nlargest(5, predictions) \n",
    "    \n",
    "    largest_indices = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        largest_indices.append(list(predictions).index(largest[i]))\n",
    "        \n",
    "    true_labels = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        for label in labels_dict:\n",
    "            if labels_dict[label][0] == largest_indices[i]:\n",
    "                true_labels.append(label)\n",
    "                \n",
    "    return true_labels            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting train data\n",
    "\n",
    "def get_data():\n",
    "    \n",
    "    input_train_file = \"train.csv\"\n",
    "    train_dataframe = pd.read_csv(input_train_file, header = 0)\n",
    "    train_labels = train_dataframe[\"Id\"].as_matrix()\n",
    "\n",
    "    labels_dict = {}\n",
    "    counter = 0\n",
    "    for i in range(9850):\n",
    "        if train_labels[i] in labels_dict:\n",
    "            labels_dict[train_labels[i]][1] += 1\n",
    "        else:\n",
    "            labels_dict[train_labels[i]] = [counter, 1]\n",
    "            counter += 1\n",
    "\n",
    "    img_list = []\n",
    "    for img_name in (os.listdir(\"new_train\")):\n",
    "        img_list.append(\"new_train/\" + img_name)\n",
    "\n",
    "    indices_list = []\n",
    "    for j in range(9850):\n",
    "        indices_list.append(labels_dict[train_labels[j]][0])\n",
    "\n",
    "    one_hot_labels = tf.one_hot(indices_list, 4251) \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        one_hot_labels = one_hot_labels.eval()\n",
    "    \n",
    "    def _parse_function(filename, label):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_jpeg(image_string)\n",
    "        image_resized = tf.image.resize_images(image_decoded, [512, 512])\n",
    "        return image_resized, label\n",
    "\n",
    "    filenames = tf.constant(img_list)\n",
    "\n",
    "    labels = tf.constant(indices_list)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    \n",
    "    return dataset, one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting test data\n",
    "\n",
    "def get_test_data():\n",
    "    \n",
    "    img_list = []\n",
    "    for img_name in (os.listdir(\"new_test\")):\n",
    "        img_list.append(\"new_test/\" + img_name)\n",
    "    \n",
    "    def _parse_function(filename):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_jpeg(image_string)\n",
    "        image_resized = tf.image.resize_images(image_decoded, [512, 512])\n",
    "        return image_resized\n",
    "    \n",
    "    filenames = tf.constant(img_list)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames))\n",
    "\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model \n",
    "\n",
    "def model():\n",
    "    \n",
    "    features = tf.placeholder(tf.float32, shape = (50, 512, 512, 3), name = \"features\")\n",
    "    labels = tf.placeholder(tf.float32, shape = (50, 4251), name = \"labels\")\n",
    "    \n",
    "    input_layer = tf.reshape(features, (50, 512, 512, 3))\n",
    "    \n",
    "    conv1 = tf.contrib.layers.conv2d(\n",
    "    inputs = input_layer,\n",
    "    num_outputs = 16,\n",
    "    kernel_size = [3 , 3],\n",
    "    stride = 1,\n",
    "    padding = 'SAME')\n",
    "    \n",
    "    pool1 = tf.contrib.layers.max_pool2d(\n",
    "    inputs = conv1,\n",
    "    kernel_size = [3 , 3],\n",
    "    stride = 1,\n",
    "    padding = 'VALID')\n",
    "    \n",
    "    conv2 = tf.contrib.layers.conv2d(\n",
    "    inputs = pool1,\n",
    "    num_outputs = 32,\n",
    "    kernel_size = [5 , 5],\n",
    "    stride = 3,\n",
    "    padding = 'SAME')\n",
    "    \n",
    "    pool2 = tf.contrib.layers.max_pool2d(\n",
    "    inputs = conv2,\n",
    "    kernel_size = [3 , 3],\n",
    "    stride = 1,\n",
    "    padding = 'VALID')\n",
    "    \n",
    "    conv3 = tf.contrib.layers.conv2d(\n",
    "    inputs = pool2,\n",
    "    num_outputs = 64,\n",
    "    kernel_size = [7 , 7],\n",
    "    stride = 5,\n",
    "    padding = 'SAME')\n",
    "    \n",
    "    pool3 = tf.contrib.layers.max_pool2d(\n",
    "    inputs = conv3,\n",
    "    kernel_size = [3 , 3],\n",
    "    stride = 1,\n",
    "    padding = 'VALID')\n",
    "    \n",
    "    conv4 = tf.contrib.layers.conv2d(\n",
    "    inputs = pool3,\n",
    "    num_outputs = 128,\n",
    "    kernel_size = [9 , 9],\n",
    "    stride = 7,\n",
    "    padding = 'SAME')\n",
    "    \n",
    "    pool4 = tf.contrib.layers.max_pool2d(\n",
    "    inputs = conv4,\n",
    "    kernel_size = [3 , 3],\n",
    "    stride = 1,\n",
    "    padding = 'VALID')\n",
    "    \n",
    "    flatten_layer = tf.contrib.layers.flatten(pool4)\n",
    "    \n",
    "    fc1 = tf.contrib.layers.fully_connected(flatten_layer, 10000)\n",
    "    fc2 = tf.contrib.layers.dropout(fc1, keep_prob = 0.8)\n",
    "    fc3 = tf.contrib.layers.fully_connected(fc2, 6000)\n",
    "    pred = tf.contrib.layers.fully_connected(fc3, 4251)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(labels, pred)\n",
    "    \n",
    "    return features, labels, pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the model\n",
    "\n",
    "def run_model(num_epochs):\n",
    "    \n",
    "    test_img_list =  os.listdir(\"new_test\")\n",
    "    \n",
    "    dataset1, one_hot_labels = get_data()\n",
    "    \n",
    "    features, labels, pred, loss = model()\n",
    "    \n",
    "    dataset1 = dataset1.repeat().batch(50)\n",
    "    \n",
    "    iter = dataset1.make_one_shot_iterator()\n",
    "    \n",
    "    x, y = iter.get_next()\n",
    "    \n",
    "    dataset2 = get_test_data()\n",
    "    \n",
    "    dataset2 = dataset2.repeat().batch(50)\n",
    "    \n",
    "    iter = dataset2.make_one_shot_iterator()\n",
    "    \n",
    "    x_test = iter.get_next()\n",
    "         \n",
    "    optimizer = tf.train.MomentumOptimizer(0.001, 0.9).minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        for i in range(num_epochs):\n",
    "            for j in range(197):\n",
    "                new_x =x.eval()\n",
    "                feed_dict = {features: new_x, labels: one_hot_labels[j * 50 : (j+1) * 50][:]}\n",
    "                _, loss_value = sess.run([optimizer, loss], feed_dict)\n",
    "                print(\"Epoch: {}, Iter: {}, Loss: {:.4f}\".format(i, j, loss_value))\n",
    "        \n",
    "        save_path = saver.save(sess, \"model/hbw_model.ckpt\")\n",
    "        \n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "        df = pd.DataFrame([], columns=['Image', ' Id'])\n",
    "        m = 0\n",
    "        \n",
    "        for j in range(313):\n",
    "            new_x_test = x_test.eval()\n",
    "            feed_dict_test = {features: new_x_test}\n",
    "            prediction = sess.run(pred, feed_dict)\n",
    "            for k in range(len(prediction)):\n",
    "                labels = get_labels(prediction[k])\n",
    "                if m < len(test_img_list):\n",
    "                    img_name = test_img_list[m]\n",
    "                else:\n",
    "                    break\n",
    "                df = df.append({'Image': img_name, 'Id': labels[0] + \" \" + labels[1] + \" \" + labels[2] + \" \" + labels[3] + \" \" + labels[4]}, ignore_index=True)\n",
    "                m += 1\n",
    "        \n",
    "        df.to_csv('outputs.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

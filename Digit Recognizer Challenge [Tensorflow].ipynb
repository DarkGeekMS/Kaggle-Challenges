{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split  #for doing train/validation split of training data\n",
    "\n",
    "from mnist import MNIST  #for reading mnist full test data to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(\"label\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.as_matrix()\n",
    "labels = labels.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing images from 784 X 1 to 28 X 28 X 1\n",
    "\n",
    "resized_train_data = np.zeros((len(train_data), 28, 28, 1))\n",
    "for i in range(len(train_data)):\n",
    "    resized_train_data[i] = train_data[i].reshape((28 , 28 , 1))\n",
    "resized_train_data = resized_train_data / 255.0    # normalizing input data between 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding the labels\n",
    "\n",
    "one_hot_labels = np.zeros((len(train_data), 10))\n",
    "for i in range(len(train_data)):\n",
    "    one_hot_labels[i][labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing train/validation split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(resized_train_data, one_hot_labels, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building basic CNN model using tf.contrib.layers modules\n",
    "\n",
    "def build_model():\n",
    "        \n",
    "    features = tf.placeholder(tf.float32, shape = (None, 28, 28, 1), name = \"features\")\n",
    "    labels = tf.placeholder(tf.float32, shape = (None, 10), name = \"labels\")\n",
    "    \n",
    "    conv1 = tf.contrib.layers.conv2d(\n",
    "    inputs = features,\n",
    "    num_outputs = 32,\n",
    "    kernel_size = [3 , 3],\n",
    "    stride = 1,\n",
    "    padding = 'SAME')\n",
    "    \n",
    "    pool1 = tf.contrib.layers.dropout(conv1, keep_prob = 0.5)\n",
    "    \n",
    "    pool1 = tf.contrib.layers.max_pool2d(\n",
    "    inputs = pool1,\n",
    "    kernel_size = [2 , 2],\n",
    "    stride = 2,\n",
    "    padding = 'VALID')\n",
    "    \n",
    "    conv2 = tf.contrib.layers.conv2d(\n",
    "    inputs = pool1,\n",
    "    num_outputs = 64,\n",
    "    kernel_size = [3 , 3],\n",
    "    stride = 1,\n",
    "    padding = 'SAME')\n",
    "    \n",
    "    pool2 = tf.contrib.layers.dropout(conv2, keep_prob = 0.5)\n",
    "    \n",
    "    pool2 = tf.contrib.layers.max_pool2d(\n",
    "    inputs = pool2,\n",
    "    kernel_size = [2 , 2],\n",
    "    stride = 2,\n",
    "    padding = 'VALID')\n",
    "    \n",
    "    conv3 = tf.contrib.layers.conv2d(\n",
    "    inputs = pool2,\n",
    "    num_outputs = 64,\n",
    "    kernel_size = [3 , 3],\n",
    "    stride = 1,\n",
    "    padding = 'SAME')\n",
    "    \n",
    "    pool3 = tf.contrib.layers.dropout(conv3, keep_prob = 0.5)\n",
    "    \n",
    "    flatten_layer = tf.contrib.layers.flatten(pool3)\n",
    "    \n",
    "    fc1 = tf.contrib.layers.fully_connected(flatten_layer, 64)\n",
    "    \n",
    "    fc2 = tf.contrib.layers.fully_connected(fc1, 10, activation_fn = None)\n",
    "    \n",
    "    pred = tf.nn.softmax(fc2)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(labels, fc2)\n",
    "    \n",
    "    accuracy = 100 * tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(pred, 1), tf.argmax(labels, 1))))\n",
    "    \n",
    "    return features, labels, pred, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training model and performing validation every 10 epochs \n",
    "\n",
    "features, labels, pred, loss, accuracy = build_model()\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    for j in range(int(len(X_train)/batch_size)):\n",
    "\n",
    "        feed_dict = {features: X_train[j * batch_size : (j+1) * batch_size],\n",
    "                     labels: y_train[j * batch_size : (j+1) * batch_size]}\n",
    "\n",
    "        loss_value, acc, _ = sess.run([loss, accuracy, optimizer], feed_dict)\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.4f}\".format(i + 1, loss_value), \", Accuracy = {:.4f}\".format(acc))\n",
    "        \n",
    "    if not ((i+1) % 10):\n",
    "        \n",
    "        feed_dict = {features: X_val, labels: y_val}\n",
    "        \n",
    "        val_loss_value, val_acc = sess.run([loss, accuracy], feed_dict)\n",
    "        \n",
    "        print(\"Epoch: {}: Validation Loss = {:.4f}\".format(i + 1, val_loss_value), \", Validation Accuracy = {:.4f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading full mnist test data to perform testing\n",
    "\n",
    "mndata = MNIST('samples')\n",
    "\n",
    "t_images, t_labels = mndata.load_testing()\n",
    "\n",
    "t_images = np.asarray(t_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = np.zeros((len(t_images), 28, 28, 1))\n",
    "for i in range(len(t_images)):\n",
    "    mnist_test[i] = t_images[i].reshape((28 , 28 , 1))\n",
    "mnist_test = mnist_test / 255.0    \n",
    "mnist_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_mnist = np.zeros((len(t_images), 10))\n",
    "for i in range(len(t_images)):\n",
    "    oh_mnist[i][t_labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "predictions = np.zeros((len(t_images), 10))\n",
    "\n",
    "for i in range(100): # inference on 100 batch size\n",
    "\n",
    "    feed_dict = {features: mnist_test[i * 100 : (i + 1) * 100], labels: oh_mnist[i * 100 : (i + 1) * 100]}\n",
    "\n",
    "    acc, predicts = sess.run([accuracy, pred], feed_dict)\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    predictions[i * 100 : (i + 1) * 100] = predicts\n",
    "\n",
    "print(\"MNIST Test Accuracy = {:.4f}\".format(np.mean(accuracies)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, 1)\n",
    "oh_mnist = np.argmax(oh_mnist, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = tf.confusion_matrix(oh_mnist, predictions) # creating confusion matrix on full mnist test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

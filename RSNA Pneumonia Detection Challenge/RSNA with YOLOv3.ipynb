{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "% matplotlib inline\n",
    "\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolder(Dataset):\n",
    "    def __init__(self, folder_path, img_size=512):\n",
    "        self.files = sorted(glob.glob('%s/*.*' % folder_path))\n",
    "        self.img_shape = (img_size, img_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.files[index % len(self.files)]\n",
    "        # Extract image\n",
    "        img = np.array(Image.open(img_path))\n",
    "        img = img.reshape((img.shape[0], img.shape[1], 1))\n",
    "        h, w, _ = img.shape\n",
    "        dim_diff = np.abs(h - w)\n",
    "        # Upper (left) and lower (right) padding\n",
    "        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
    "        # Determine padding\n",
    "        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n",
    "        # Add padding\n",
    "        input_img = np.pad(img, pad, 'constant', constant_values=127.5) / 255.\n",
    "        # Resize and normalize\n",
    "        input_img = resize(input_img, (*self.img_shape, 1), mode='reflect')\n",
    "        # Channels-first\n",
    "        input_img = np.transpose(input_img, (2, 0, 1))\n",
    "        # As pytorch tensor\n",
    "        input_img = torch.from_numpy(input_img).float()\n",
    "\n",
    "        return img_path, input_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset):\n",
    "    def __init__(self, list_path, img_size=512):\n",
    "        with open(list_path, 'r') as file:\n",
    "            self.img_files = file.readlines()\n",
    "        self.label_files = [path.replace('images_jpeg', 'labels').replace('.png', '.txt').replace('.jpg', '.txt')\n",
    "                            for path in self.img_files]\n",
    "        self.img_shape = (img_size, img_size)\n",
    "        self.max_objects = 50\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #---------\n",
    "        #  Image\n",
    "        #---------\n",
    "\n",
    "        img_path = self.img_files[index % len(self.img_files)].rstrip()\n",
    "        img = np.array(Image.open(img_path))\n",
    "        img = img.reshape((img.shape[0], img.shape[1], 1))\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        dim_diff = np.abs(h - w)\n",
    "        # Upper (left) and lower (right) padding\n",
    "        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
    "        # Determine padding\n",
    "        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n",
    "        # Add padding\n",
    "        input_img = np.pad(img, pad, 'constant', constant_values=128) / 255.\n",
    "        padded_h, padded_w, _ = input_img.shape\n",
    "        # Resize and normalize\n",
    "        input_img = resize(input_img, (*self.img_shape, 1), mode='reflect')\n",
    "        # Channels-first\n",
    "        input_img = np.transpose(input_img, (2, 0, 1))\n",
    "        # As pytorch tensor\n",
    "        input_img = torch.from_numpy(input_img).float()\n",
    "\n",
    "        #---------\n",
    "        #  Label\n",
    "        #---------\n",
    "\n",
    "        label_path = self.label_files[index % len(self.img_files)].rstrip()\n",
    "\n",
    "        labels = None\n",
    "        if os.path.exists(label_path):\n",
    "            labels = np.loadtxt(label_path).reshape(-1, 5)\n",
    "            labels = np.divide(labels, 1024)\n",
    "            # Extract coordinates for unpadded + unscaled image\n",
    "            x1 = w * (labels[:, 1] - labels[:, 3]/2)\n",
    "            y1 = h * (labels[:, 2] - labels[:, 4]/2)\n",
    "            x2 = w * (labels[:, 1] + labels[:, 3]/2)\n",
    "            y2 = h * (labels[:, 2] + labels[:, 4]/2)\n",
    "            # Adjust for added padding\n",
    "            x1 += pad[1][0]\n",
    "            y1 += pad[0][0]\n",
    "            x2 += pad[1][0]\n",
    "            y2 += pad[0][0]\n",
    "            # Calculate ratios from coordinates\n",
    "            labels[:, 1] = ((x1 + x2) / 2) / padded_w\n",
    "            labels[:, 2] = ((y1 + y2) / 2) / padded_h\n",
    "            labels[:, 3] *= w / padded_w\n",
    "            labels[:, 4] *= h / padded_h\n",
    "        # Fill matrix\n",
    "        filled_labels = np.zeros((self.max_objects, 5))\n",
    "        if labels is not None:\n",
    "            filled_labels[range(len(labels))[:self.max_objects]] = labels[:self.max_objects]\n",
    "        filled_labels = torch.from_numpy(filled_labels)\n",
    "\n",
    "        return img_path, input_img, filled_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_config(path):\n",
    "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
    "    file = open(path, 'r')\n",
    "    lines = file.read().split('\\n')\n",
    "    lines = [x for x in lines if x and not x.startswith('#')]\n",
    "    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n",
    "    module_defs = []\n",
    "    for line in lines:\n",
    "        if line.startswith('['): # This marks the start of a new block\n",
    "            module_defs.append({})\n",
    "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
    "            if module_defs[-1]['type'] == 'convolutional':\n",
    "                module_defs[-1]['batch_normalize'] = 0\n",
    "        else:\n",
    "            key, value = line.split(\"=\")\n",
    "            value = value.strip()\n",
    "            module_defs[-1][key.rstrip()] = value.strip()\n",
    "\n",
    "    return module_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_config(path):\n",
    "    \"\"\"Parses the data configuration file\"\"\"\n",
    "    options = dict()\n",
    "    options['gpus'] = '0,1,2,3'\n",
    "    options['num_workers'] = '10'\n",
    "    with open(path, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '' or line.startswith('#'):\n",
    "            continue\n",
    "        key, value = line.split('=')\n",
    "        options[key.strip()] = value.strip()\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    Loads class labels at 'path'\n",
    "    \"\"\"\n",
    "    fp = open(path, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area =    torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * \\\n",
    "                    torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):\n",
    "    \"\"\"\n",
    "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "    Non-Maximum Suppression to further filter detections.\n",
    "    Returns detections with shape:\n",
    "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
    "    \"\"\"\n",
    "\n",
    "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()\n",
    "        image_pred = image_pred[conf_mask]\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Get score and class with highest confidence\n",
    "        class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1,  keepdim=True)\n",
    "        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)\n",
    "        # Iterate through all predicted classes\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        if prediction.is_cuda:\n",
    "            unique_labels = unique_labels.cuda()\n",
    "        for c in unique_labels:\n",
    "            # Get the detections with the particular class\n",
    "            detections_class = detections[detections[:, -1] == c]\n",
    "            # Sort the detections by maximum objectness confidence\n",
    "            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)\n",
    "            detections_class = detections_class[conf_sort_index]\n",
    "            # Perform non-maximum suppression\n",
    "            max_detections = []\n",
    "            while detections_class.size(0):\n",
    "                # Get detection with highest confidence and save as max detection\n",
    "                max_detections.append(detections_class[0].unsqueeze(0))\n",
    "                # Stop if we're at the last detection\n",
    "                if len(detections_class) == 1:\n",
    "                    break\n",
    "                # Get the IOUs for all boxes with lower confidence\n",
    "                ious = bbox_iou(max_detections[-1], detections_class[1:])\n",
    "                # Remove detections with IoU >= NMS threshold\n",
    "                detections_class = detections_class[1:][ious < nms_thres]\n",
    "\n",
    "            max_detections = torch.cat(max_detections).data\n",
    "            # Add max detections to outputs\n",
    "            output[image_i] = max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets(pred_boxes, target, anchors, num_anchors, num_classes, dim, ignore_thres, img_dim):\n",
    "    nB = target.size(0)\n",
    "    nA = num_anchors\n",
    "    nC = num_classes\n",
    "    dim = dim\n",
    "    mask        = torch.zeros(nB, nA, dim, dim)\n",
    "    conf_mask   = torch.ones(nB, nA, dim, dim)\n",
    "    tx          = torch.zeros(nB, nA, dim, dim)\n",
    "    ty          = torch.zeros(nB, nA, dim, dim)\n",
    "    tw          = torch.zeros(nB, nA, dim, dim)\n",
    "    th          = torch.zeros(nB, nA, dim, dim)\n",
    "    tconf       = torch.zeros(nB, nA, dim, dim)\n",
    "    tcls        = torch.zeros(nB, nA, dim, dim, num_classes)\n",
    "\n",
    "    nGT = 0\n",
    "    nCorrect = 0\n",
    "    for b in range(nB):  # Batch size\n",
    "        for t in range(target.shape[1]):\n",
    "            if target[b, t].sum() == 0:\n",
    "                continue\n",
    "            nGT += 1\n",
    "            # Convert to position relative to box\n",
    "            gx = target[b, t, 1] * dim\n",
    "            gy = target[b, t, 2] * dim\n",
    "            gw = target[b, t, 3] * dim\n",
    "            gh = target[b, t, 4] * dim\n",
    "            # Get grid box indices\n",
    "            gi = int(gx)\n",
    "            gj = int(gy)\n",
    "            # Get shape of gt box\n",
    "            gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0)\n",
    "            # Get shape of anchor box\n",
    "            anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((len(anchors), 2)), np.array(anchors)), 1))\n",
    "            # Calculate iou between gt and anchor shapes\n",
    "            anch_ious = bbox_iou(gt_box, anchor_shapes)\n",
    "            # Where the overlap is larger than threshold set mask to zero (ignore)\n",
    "            conf_mask[b, anch_ious > ignore_thres] = 0\n",
    "            # Find the best matching anchor box\n",
    "            best_n = np.argmax(anch_ious)\n",
    "            # Get ground truth box\n",
    "            gt_box = torch.FloatTensor(np.array([gx, gy, gw, gh])).unsqueeze(0)\n",
    "            # Get the best prediction\n",
    "            pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)\n",
    "            # Masks\n",
    "            mask[b, best_n, gj, gi] = 1\n",
    "            conf_mask[b, best_n, gj, gi] = 1\n",
    "            # Coordinates\n",
    "            tx[b, best_n, gj, gi] = gx - gi\n",
    "            ty[b, best_n, gj, gi] = gy - gj\n",
    "            # Width and height\n",
    "            tw[b, best_n, gj, gi] = math.log(gw/anchors[best_n][0] + 1e-16)\n",
    "            th[b, best_n, gj, gi] = math.log(gh/anchors[best_n][1] + 1e-16)\n",
    "            # One-hot encoding of label\n",
    "            tcls[b, best_n, gj, gi, int(target[b, t, 0])] = 1\n",
    "            # Calculate iou between ground truth and best matching prediction\n",
    "            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n",
    "            tconf[b, best_n, gj, gi] = 1\n",
    "\n",
    "            if iou > 0.5:\n",
    "                nCorrect += 1\n",
    "\n",
    "    return nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return torch.from_numpy(np.eye(num_classes, dtype='uint8')[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modules(module_defs):\n",
    "    \"\"\"\n",
    "    Constructs module list of layer blocks from module configuration in module_defs\n",
    "    \"\"\"\n",
    "    hyperparams = module_defs.pop(0)\n",
    "    output_filters = [int(hyperparams['channels'])]\n",
    "    module_list = nn.ModuleList()\n",
    "    for i, module_def in enumerate(module_defs):\n",
    "        modules = nn.Sequential()\n",
    "\n",
    "        if module_def['type'] == 'convolutional':\n",
    "            bn = int(module_def['batch_normalize'])\n",
    "            filters = int(module_def['filters'])\n",
    "            kernel_size = int(module_def['size'])\n",
    "            pad = (kernel_size - 1) // 2 if int(module_def['pad']) else 0\n",
    "            modules.add_module('conv_%d' % i, nn.Conv2d(in_channels=output_filters[-1],\n",
    "                                                        out_channels=filters,\n",
    "                                                        kernel_size=kernel_size,\n",
    "                                                        stride=int(module_def['stride']),\n",
    "                                                        padding=pad,\n",
    "                                                        bias=not bn))\n",
    "            if bn:\n",
    "                modules.add_module('batch_norm_%d' % i, nn.BatchNorm2d(filters))\n",
    "            if module_def['activation'] == 'leaky':\n",
    "                modules.add_module('leaky_%d' % i, nn.LeakyReLU(0.1))\n",
    "\n",
    "        elif module_def['type'] == 'upsample':\n",
    "            upsample = nn.Upsample( scale_factor=int(module_def['stride']),\n",
    "                                    mode='nearest')\n",
    "            modules.add_module('upsample_%d' % i, upsample)\n",
    "\n",
    "        elif module_def['type'] == 'route':\n",
    "            layers = [int(x) for x in module_def[\"layers\"].split(',')]\n",
    "            filters = sum([output_filters[layer_i] for layer_i in layers])\n",
    "            modules.add_module('route_%d' % i, EmptyLayer())\n",
    "\n",
    "        elif module_def['type'] == 'shortcut':\n",
    "            filters = output_filters[int(module_def['from'])]\n",
    "            modules.add_module(\"shortcut_%d\" % i, EmptyLayer())\n",
    "\n",
    "        elif module_def[\"type\"] == \"yolo\":\n",
    "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
    "            # Extract anchors\n",
    "            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
    "            anchors = [anchors[i] for i in anchor_idxs]\n",
    "            num_classes = int(module_def['classes'])\n",
    "            img_height = int(hyperparams['height'])\n",
    "            # Define detection layer\n",
    "            yolo_layer = YOLOLayer(anchors, num_classes, img_height)\n",
    "            modules.add_module('yolo_%d' % i, yolo_layer)\n",
    "        # Register module list and number of output filters\n",
    "        module_list.append(modules)\n",
    "        output_filters.append(filters)\n",
    "\n",
    "    return hyperparams, module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyLayer(nn.Module):\n",
    "    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLayer(nn.Module):\n",
    "    \"\"\"Detection layer\"\"\"\n",
    "    def __init__(self, anchors, num_classes, img_dim):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.bbox_attrs = 5 + num_classes\n",
    "        self.img_dim = img_dim\n",
    "        self.ignore_thres = 0.5\n",
    "        self.lambda_coord = 1\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        bs = x.size(0)\n",
    "        g_dim = x.size(2)\n",
    "        stride =  self.img_dim / g_dim\n",
    "        # Tensors for cuda support\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "\n",
    "        prediction = x.view(bs,  self.num_anchors, self.bbox_attrs, g_dim, g_dim).permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "        # Get outputs\n",
    "        x = torch.sigmoid(prediction[..., 0])          # Center x\n",
    "        y = torch.sigmoid(prediction[..., 1])          # Center y\n",
    "        w = prediction[..., 2]                         # Width\n",
    "        h = prediction[..., 3]                         # Height\n",
    "        conf = torch.sigmoid(prediction[..., 4])       # Conf\n",
    "        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n",
    "\n",
    "        # Calculate offsets for each grid\n",
    "        grid_x = torch.linspace(0, g_dim-1, g_dim).repeat(g_dim,1).repeat(bs*self.num_anchors, 1, 1).view(x.shape).type(FloatTensor)\n",
    "        grid_y = torch.linspace(0, g_dim-1, g_dim).repeat(g_dim,1).t().repeat(bs*self.num_anchors, 1, 1).view(y.shape).type(FloatTensor)\n",
    "        scaled_anchors = [(a_w / stride, a_h / stride) for a_w, a_h in self.anchors]\n",
    "        anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))\n",
    "        anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))\n",
    "        anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, g_dim*g_dim).view(w.shape)\n",
    "        anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, g_dim*g_dim).view(h.shape)\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        pred_boxes = FloatTensor(prediction[..., :4].shape)\n",
    "        pred_boxes[..., 0] = x.data + grid_x\n",
    "        pred_boxes[..., 1] = y.data + grid_y\n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\n",
    "\n",
    "        # Training\n",
    "        if targets is not None:\n",
    "\n",
    "            if x.is_cuda:\n",
    "                self.mse_loss = self.mse_loss.cuda()\n",
    "                self.bce_loss = self.bce_loss.cuda()    \n",
    "\n",
    "            nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls = build_targets(pred_boxes.cpu().data,\n",
    "                                                                            targets.cpu().data,\n",
    "                                                                            scaled_anchors,\n",
    "                                                                            self.num_anchors,\n",
    "                                                                            self.num_classes,\n",
    "                                                                            g_dim,\n",
    "                                                                            self.ignore_thres,\n",
    "                                                                            self.img_dim)\n",
    "\n",
    "            nProposals = int((conf > 0.25).sum().item())\n",
    "            recall = float(nCorrect / nGT) if nGT else 1\n",
    "\n",
    "            # Handle masks\n",
    "            mask = Variable(mask.type(FloatTensor))\n",
    "            cls_mask = Variable(mask.unsqueeze(-1).repeat(1, 1, 1, 1, self.num_classes).type(FloatTensor))\n",
    "            conf_mask = Variable(conf_mask.type(FloatTensor))\n",
    "\n",
    "            # Handle target variables\n",
    "            tx    = Variable(tx.type(FloatTensor), requires_grad=False)\n",
    "            ty    = Variable(ty.type(FloatTensor), requires_grad=False)\n",
    "            tw    = Variable(tw.type(FloatTensor), requires_grad=False)\n",
    "            th    = Variable(th.type(FloatTensor), requires_grad=False)\n",
    "            tconf = Variable(tconf.type(FloatTensor), requires_grad=False)\n",
    "            tcls  = Variable(tcls.type(FloatTensor), requires_grad=False)\n",
    "\n",
    "            # Mask outputs to ignore non-existing objects\n",
    "            loss_x = self.lambda_coord * self.bce_loss(x * mask, tx * mask)\n",
    "            loss_y = self.lambda_coord * self.bce_loss(y * mask, ty * mask)\n",
    "            loss_w = self.lambda_coord * self.mse_loss(w * mask, tw * mask) / 2\n",
    "            loss_h = self.lambda_coord * self.mse_loss(h * mask, th * mask) / 2\n",
    "            loss_conf = self.bce_loss(conf * conf_mask, tconf * conf_mask)\n",
    "            loss_cls = self.bce_loss(pred_cls * cls_mask, tcls * cls_mask)\n",
    "            loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
    "\n",
    "            return loss, loss_x.item(), loss_y.item(), loss_w.item(), loss_h.item(), loss_conf.item(), loss_cls.item(), recall\n",
    "\n",
    "        else:\n",
    "            # If not in training phase return predictions\n",
    "            output = torch.cat((pred_boxes.view(bs, -1, 4) * stride, conf.view(bs, -1, 1), pred_cls.view(bs, -1, self.num_classes)), -1)\n",
    "            return output.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "    \"\"\"YOLOv3 object detection model\"\"\"\n",
    "    def __init__(self, config_path, img_size=512):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.module_defs = parse_model_config(config_path)\n",
    "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
    "        self.img_size = img_size\n",
    "        self.seen = 0\n",
    "        self.header_info = np.array([0, 0, 0, self.seen, 0])\n",
    "        self.loss_names = ['x', 'y', 'w', 'h', 'conf', 'cls', 'recall']\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        is_training = targets is not None\n",
    "        output = []\n",
    "        self.losses = defaultdict(float)\n",
    "        layer_outputs = []\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if module_def['type'] in ['convolutional', 'upsample']:\n",
    "                x = module(x)\n",
    "            elif module_def['type'] == 'route':\n",
    "                layer_i = [int(x) for x in module_def['layers'].split(',')]\n",
    "                x = torch.cat([layer_outputs[i] for i in layer_i], 1)\n",
    "            elif module_def['type'] == 'shortcut':\n",
    "                layer_i = int(module_def['from'])\n",
    "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
    "            elif module_def['type'] == 'yolo':\n",
    "                # Train phase: get loss\n",
    "                if is_training:\n",
    "                    x, *losses = module[0](x, targets)\n",
    "                    for name, loss in zip(self.loss_names, losses):\n",
    "                        self.losses[name] += loss\n",
    "                # Test phase: Get detections\n",
    "                else:\n",
    "                    x = module(x)\n",
    "                output.append(x)\n",
    "            layer_outputs.append(x)\n",
    "\n",
    "        self.losses['recall'] /= 3\n",
    "        return sum(output) if is_training else torch.cat(output, 1)\n",
    "\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
    "\n",
    "        #Open the weights file\n",
    "        fp = open(weights_path, \"rb\")\n",
    "        header = np.fromfile(fp, dtype=np.int32, count=5)   # First five are header values\n",
    "\n",
    "        # Needed to write header when saving weights\n",
    "        self.header_info = header\n",
    "\n",
    "        self.seen = header[3]\n",
    "        weights = np.fromfile(fp, dtype=np.float32)         # The rest are weights\n",
    "        fp.close()\n",
    "\n",
    "        ptr = 0\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if module_def['type'] == 'convolutional':\n",
    "                conv_layer = module[0]\n",
    "                if module_def['batch_normalize']:\n",
    "                    # Load BN bias, weights, running mean and running variance\n",
    "                    bn_layer = module[1]\n",
    "                    num_b = bn_layer.bias.numel() # Number of biases\n",
    "                    # Bias\n",
    "                    bn_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.bias)\n",
    "                    bn_layer.bias.data.copy_(bn_b)\n",
    "                    ptr += num_b\n",
    "                    # Weight\n",
    "                    bn_w = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.weight)\n",
    "                    bn_layer.weight.data.copy_(bn_w)\n",
    "                    ptr += num_b\n",
    "                    # Running Mean\n",
    "                    bn_rm = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_mean)\n",
    "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
    "                    ptr += num_b\n",
    "                    # Running Var\n",
    "                    bn_rv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_var)\n",
    "                    bn_layer.running_var.data.copy_(bn_rv)\n",
    "                    ptr += num_b\n",
    "                else:\n",
    "                    # Load conv. bias\n",
    "                    num_b = conv_layer.bias.numel()\n",
    "                    conv_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(conv_layer.bias)\n",
    "                    conv_layer.bias.data.copy_(conv_b)\n",
    "                    ptr += num_b\n",
    "                # Load conv. weights\n",
    "                num_w = conv_layer.weight.numel()\n",
    "                conv_w = torch.from_numpy(weights[ptr:ptr + num_w]).view_as(conv_layer.weight)\n",
    "                conv_layer.weight.data.copy_(conv_w)\n",
    "                ptr += num_w\n",
    "\n",
    "    \"\"\"\n",
    "        @:param path    - path of the new weights file\n",
    "        @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n",
    "    \"\"\"\n",
    "    def save_weights(self, path, cutoff=-1):\n",
    "\n",
    "        fp = open(path, 'wb')\n",
    "        self.header_info[3] = self.seen\n",
    "        self.header_info.tofile(fp)\n",
    "\n",
    "        # Iterate through layers\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
    "            if module_def['type'] == 'convolutional':\n",
    "                conv_layer = module[0]\n",
    "                # If batch norm, load bn first\n",
    "                if module_def['batch_normalize']:\n",
    "                    bn_layer = module[1]\n",
    "                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv bias\n",
    "                else:\n",
    "                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv weights\n",
    "                conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arg = dict()\n",
    "\n",
    "train_arg[\"epochs\"] = 50\n",
    "train_arg[\"image_folder\"] = \"data/stage_1_train_images_jpeg/train\"\n",
    "train_arg[\"batch_size\"] = 4\n",
    "train_arg[\"model_config_path\"] = \"data/yolov3.cfg\"\n",
    "train_arg[\"data_config_path\"] = \"data/rsna.data\"\n",
    "train_arg[\"class_path\"] = \"data/rsna.names\"\n",
    "train_arg[\"conf_thres\"] = 0.8\n",
    "train_arg[\"nms_thres\"] = 0.4\n",
    "train_arg[\"n_cpu\"] = 0\n",
    "train_arg[\"img_size\"] = 512\n",
    "train_arg[\"checkpoint_interval\"] = 1\n",
    "train_arg[\"checkpoint_dir\"] = \"checkpoints\"\n",
    "train_arg[\"use_cuda\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available() and train_arg[\"use_cuda\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = load_classes(train_arg[\"class_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data configuration\n",
    "data_config     = parse_data_config(train_arg[\"data_config_path\"])\n",
    "train_path      = data_config['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hyper parameters\n",
    "hyperparams     = parse_model_config(train_arg[\"model_config_path\"])[0]\n",
    "learning_rate   = float(hyperparams['learning_rate'])\n",
    "momentum        = float(hyperparams['momentum'])\n",
    "decay           = float(hyperparams['decay'])\n",
    "burn_in         = int(hyperparams['burn_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model\n",
    "model = Darknet(train_arg[\"model_config_path\"])\n",
    "model.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    ListDataset(train_path),\n",
    "    batch_size=train_arg[\"batch_size\"], shuffle=False, num_workers=train_arg[\"n_cpu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, dampening=0, weight_decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(train_arg[\"epochs\"]):\n",
    "    for batch_i, (_, imgs, targets) in enumerate(dataloader):\n",
    "\n",
    "        imgs = Variable(imgs.type(Tensor))\n",
    "        targets = Variable(targets.type(Tensor), requires_grad=False)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model(imgs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        print('[Epoch %d/%d, Batch %d/%d] [Losses: x %f, y %f, w %f, h %f, conf %f, cls %f, total %f, recall: %.5f]' %\n",
    "                                    (epoch, train_arg[\"epochs\"], batch_i, len(dataloader),\n",
    "                                    model.losses['x'], model.losses['y'], model.losses['w'],\n",
    "                                    model.losses['h'], model.losses['conf'], model.losses['cls'],\n",
    "                                    loss.item(), model.losses['recall']))\n",
    "\n",
    "        model.seen += imgs.size(0)\n",
    "\n",
    "    if epoch % train_arg[\"checkpoint_interval\"] == 0:\n",
    "        model.save_weights('%s/%d.weights' % (train_arg[\"checkpoint_dir\"], epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_arg = dict()\n",
    "\n",
    "val_arg[\"batch_size\"] = 16\n",
    "val_arg[\"model_config_path\"] = \"data/yolov3.cfg\"\n",
    "val_arg[\"data_config_path\"] = \"data/rsna.data\"\n",
    "val_arg[\"class_path\"] = \"data/rsna.names\"\n",
    "val_arg[\"weights_path\"] = \"checkpoints/9.weights\"\n",
    "val_arg[\"iou_thres\"] = 0.5\n",
    "val_arg[\"conf_thres\"] = 0.5\n",
    "val_arg[\"nms_thres\"] = 0.45\n",
    "val_arg[\"n_cpu\"] = 0\n",
    "val_arg[\"img_size\"] = 512\n",
    "val_arg[\"use_cuda\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available() and val_arg[\"use_cuda\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data configuration\n",
    "data_config     = parse_data_config(val_arg[\"data_config_path\"])\n",
    "test_path       = data_config['valid']\n",
    "num_classes     = int(data_config['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model\n",
    "model = Darknet(val_arg[\"model_config_path\"])\n",
    "model.load_weights(val_arg[\"weights_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloader\n",
    "dataset = ListDataset(test_path)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "    batch_size=val_arg[\"batch_size\"], shuffle=False, num_workers=val_arg[\"n_cpu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gt = 0\n",
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Compute mAP...')\n",
    "\n",
    "outputs = []\n",
    "targets = None\n",
    "APs = []\n",
    "for batch_i, (_, imgs, targets) in enumerate(dataloader):\n",
    "    imgs = Variable(imgs.type(Tensor))\n",
    "    targets = targets.type(Tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(imgs)\n",
    "        output = non_max_suppression(output, 80, conf_thres=val_arg[\"conf_thres\"], nms_thres=val_arg[\"nms_thres\"])\n",
    "\n",
    "    # Compute average precision for each sample\n",
    "    for sample_i in range(targets.size(0)):\n",
    "        correct = []\n",
    "\n",
    "        # Get labels for sample where width is not zero (dummies)\n",
    "        annotations = targets[sample_i, targets[sample_i, :, 3] != 0]\n",
    "        # Extract detections\n",
    "        detections = output[sample_i]\n",
    "\n",
    "        if detections is None:\n",
    "            # If there are no detections but there are annotations mask as zero AP\n",
    "            if annotations.size(0) != 0:\n",
    "                APs.append(0)\n",
    "            continue\n",
    "\n",
    "        # Get detections sorted by decreasing confidence scores\n",
    "        detections = detections[np.argsort(-detections[:, 4])]\n",
    "\n",
    "        # If no annotations add number of detections as incorrect\n",
    "        if annotations.size(0) == 0:\n",
    "            correct.extend([0 for _ in range(len(detections))])\n",
    "        else:\n",
    "            # Extract target boxes as (x1, y1, x2, y2)\n",
    "            target_boxes = torch.FloatTensor(annotations[:, 1:].shape)\n",
    "            target_boxes[:, 0] = (annotations[:, 1] - annotations[:, 3] / 2)\n",
    "            target_boxes[:, 1] = (annotations[:, 2] - annotations[:, 4] / 2)\n",
    "            target_boxes[:, 2] = (annotations[:, 1] + annotations[:, 3] / 2)\n",
    "            target_boxes[:, 3] = (annotations[:, 2] + annotations[:, 4] / 2)\n",
    "            target_boxes *= val_arg[\"img_size\"]\n",
    "\n",
    "            detected = []\n",
    "            for *pred_bbox, conf, obj_conf, obj_pred in detections:\n",
    "\n",
    "                pred_bbox = torch.FloatTensor(pred_bbox).view(1, -1)\n",
    "                # Compute iou with target boxes\n",
    "                iou = bbox_iou(pred_bbox, target_boxes)\n",
    "                # Extract index of largest overlap\n",
    "                best_i = np.argmax(iou)\n",
    "                # If overlap exceeds threshold and classification is correct mark as correct\n",
    "                if iou[best_i] > val_arg[\"iou_thres\"] and obj_pred == annotations[best_i, 0] and best_i not in detected:\n",
    "                    correct.append(1)\n",
    "                    detected.append(best_i)\n",
    "                else:\n",
    "                    correct.append(0)\n",
    "\n",
    "        # Extract true and false positives\n",
    "        true_positives  = np.array(correct)\n",
    "        false_positives = 1 - true_positives\n",
    "\n",
    "        # Compute cumulative false positives and true positives\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "        # Compute recall and precision at all ranks\n",
    "        recall    = true_positives / annotations.size(0) if annotations.size(0) else true_positives\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "        # Compute average precision\n",
    "        AP = compute_ap(recall, precision)\n",
    "        APs.append(AP)\n",
    "\n",
    "        print (\"+ Sample [%d/%d] AP: %.4f (%.4f)\" % (len(APs), len(dataset), AP, np.mean(APs)))\n",
    "\n",
    "print(\"Mean Average Precision: %.4f\" % np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arg = dict()\n",
    "\n",
    "test_arg[\"image_folder\"] = \"data/stage_1_test_images_jpeg\"\n",
    "test_arg[\"batch_size\"] = 1\n",
    "test_arg[\"model_config_path\"] = \"data/yolov3.cfg\"\n",
    "test_arg[\"class_path\"] = \"data/rsna.names\"\n",
    "test_arg[\"weights_path\"] = \"checkpoints/9.weights\"\n",
    "test_arg[\"conf_thres\"] = 0.8\n",
    "test_arg[\"nms_thres\"] = 0.4\n",
    "test_arg[\"n_cpu\"] = 0\n",
    "test_arg[\"img_size\"] = 512\n",
    "test_arg[\"use_cuda\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available() and test_arg[\"use_cuda\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model = Darknet(test_arg[\"model_config_path\"], img_size=test_arg[\"img_size\"])\n",
    "model.load_weights(test_arg[\"weights_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.eval() # Set in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ImageFolder(test_arg[\"image_folder\"], img_size=test_arg[\"img_size\"]),\n",
    "                        batch_size=test_arg[\"batch_size\"], shuffle=False, num_workers=test_arg[\"n_cpu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = load_classes(test_arg[\"class_path\"]) # Extracts class labels from file\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "imgs = []           # Stores image paths\n",
    "img_detections = [] # Stores detections for each image index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('\\nPerforming object detection:')\n",
    "prev_time = time.time()\n",
    "for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n",
    "    # Configure input\n",
    "    input_imgs = Variable(input_imgs.type(Tensor))\n",
    "\n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_imgs)\n",
    "        detections = non_max_suppression(detections, 80, test_arg[\"conf_thres\"], test_arg[\"nms_thres\"])\n",
    "        \n",
    "    print(detections)\n",
    "\n",
    "    # Log progress\n",
    "    current_time = time.time()\n",
    "    inference_time = datetime.timedelta(seconds=current_time - prev_time)\n",
    "    prev_time = current_time\n",
    "    print ('\\t+ Batch %d, Inference Time: %s' % (batch_i, inference_time))\n",
    "\n",
    "    # Save image and detections\n",
    "    imgs.extend(img_paths)\n",
    "    img_detections.extend(detections)\n",
    "      \n",
    "\n",
    "# Bounding-box colors\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "print ('\\nSaving images:')\n",
    "# Iterate through images and save plot of detections\n",
    "for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n",
    "\n",
    "    print (\"(%d) Image: '%s'\" % (img_i, path))\n",
    "\n",
    "    # Create plot\n",
    "    img = np.array(Image.open(path))\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # The amount of padding that was added\n",
    "    pad_x = max(img.shape[0] - img.shape[1], 0) * (test_arg[\"img_size\"] / max(img.shape))\n",
    "    pad_y = max(img.shape[1] - img.shape[0], 0) * (test_arg[\"img_size\"] / max(img.shape))\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = test_arg[\"img_size\"] - pad_y\n",
    "    unpad_w = test_arg[\"img_size\"] - pad_x\n",
    "\n",
    "    # Draw bounding boxes and labels of detections\n",
    "    if detections is not None:\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        n_cls_preds = len(unique_labels)\n",
    "        bbox_colors = random.sample(colors, n_cls_preds)\n",
    "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "\n",
    "            print ('\\t+ Label: %s, Conf: %.5f' % (classes[int(cls_pred)], cls_conf.item()))\n",
    "\n",
    "            # Rescale coordinates to original dimensions\n",
    "            box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
    "            box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
    "            y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
    "            x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
    "\n",
    "            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "            # Create a Rectangle patch\n",
    "            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2,\n",
    "                                    edgecolor=color,\n",
    "                                    facecolor='none')\n",
    "            # Add the bbox to the plot\n",
    "            ax.add_patch(bbox)\n",
    "            # Add label\n",
    "            plt.text(x1, y1, s=classes[int(cls_pred)], color='white', verticalalignment='top',\n",
    "                    bbox={'color': color, 'pad': 0})\n",
    "\n",
    "    # Save generated image with detections\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "    plt.savefig('output/%d.png' % (img_i), bbox_inches='tight', pad_inches=0.0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
